{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data (F & L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pd_features.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(data), np\u001b[38;5;241m.\u001b[39marray(labels)\n\u001b[0;32m     17\u001b[0m output_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpd_features.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 19\u001b[0m features, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_csv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, features\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabels shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, labels\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m, in \u001b[0;36mload_features\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      7\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(file)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mnext\u001b[39m(reader)  \u001b[38;5;66;03m# Skip header\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pd_features.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load features for model training\n",
    "def load_features(file_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            features = list(map(float, row[1:-1]))  # Exclude file name and label\n",
    "            label = 0 if \"control\" in row[0].lower() else 1  # Infer label from file name\n",
    "            data.append(features)\n",
    "            labels.append(label)\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "output_csv_path = \"pd_features.csv\"\n",
    "\n",
    "features, labels = load_features(output_csv_path)\n",
    "\n",
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler()\n",
      "[[ 1.15844938  1.16824525  1.16457344  0.96544221  0.95679163  1.04682006\n",
      "   0.8560265   1.0440808   0.61159908]\n",
      " [-0.31143177 -0.30819178 -0.31050064 -0.11653894 -0.11479099 -0.11984925\n",
      "   0.04007495  0.03221609  0.00683206]\n",
      " [-0.34658811 -0.38054186 -0.3173827  -0.29616704 -0.27328537 -0.31730087\n",
      "  -0.4106434  -0.35100434 -0.41273042]\n",
      " [-0.44490609 -0.46260025 -0.41900972 -0.49055247 -0.47198142 -0.52479953\n",
      "  -0.60035119 -0.58465453 -0.6037477 ]\n",
      " [-0.25760195 -0.26869006 -0.21435435 -0.46673447 -0.43232942 -0.51427085\n",
      "  -0.66954393 -0.61398086 -0.67128743]\n",
      " [-0.78155846 -0.76444119 -0.84372301 -0.40803822 -0.39580633 -0.43731974\n",
      "  -0.42331944 -0.46058092 -0.36966991]\n",
      " [-0.71769601 -0.7268095  -0.7337055  -0.4828769  -0.46468838 -0.49841761\n",
      "  -0.62842613 -0.63055934 -0.59770511]\n",
      " [-0.39483148 -0.41020247 -0.36766174 -0.5271088  -0.49551541 -0.56058335\n",
      "  -0.70292322 -0.64738581 -0.70389557]\n",
      " [ 0.06605052  0.01593011  0.16628963  0.10039501 -0.02829809  0.27876124\n",
      "   0.7176425   0.40840002  1.04782577]\n",
      " [ 0.53693651  0.50841204  0.67182693  0.36563118  0.28677846  0.44835224\n",
      "   0.72087691  0.61199781  0.83462957]\n",
      " [ 0.93669637  0.92828307  1.00916324  0.31109563  0.22699602  0.41963834\n",
      "   0.11622431  0.02871443  0.13803818]\n",
      " [-0.02363747 -0.08962667  0.07526289  0.93207252  0.71094691  1.23684067\n",
      "   2.86993046  2.26429015  3.45925459]\n",
      " [-0.09177276 -0.12016957 -0.0809773  -0.45404076 -0.39457478 -0.52947859\n",
      "  -0.71286382 -0.62492565 -0.74311713]\n",
      " [-0.64124127 -0.63010862 -0.69043283 -0.44557674 -0.42987855 -0.46891068\n",
      "  -0.52007485 -0.55789526 -0.4374085 ]\n",
      " [ 0.17114535  0.1294982   0.26161766 -0.30425301 -0.30377023 -0.32269492\n",
      "  -0.52365897 -0.49119039 -0.51277407]\n",
      " [-0.58733736 -0.54861139 -0.64981807 -0.43961836 -0.4244449  -0.44773425\n",
      "  -0.52176207 -0.5380982  -0.46169031]\n",
      " [ 4.2496453   4.25585113  4.15623958  4.42080537  4.52854207  4.21182581\n",
      "   2.80330526  3.52236497  1.68476036]\n",
      " [-0.17529527 -0.10222878 -0.25945755 -0.26798048 -0.22248485 -0.32763287\n",
      "  -0.18441685 -0.15715487 -0.17432346]\n",
      " [ 0.07594319  0.14405443 -0.04074704 -0.20908033 -0.15198605 -0.27231024\n",
      "  -0.16624064 -0.10966498 -0.21684557]\n",
      " [ 0.33015853  0.3621316   0.31609487  0.31542017  0.20812484  0.46982229\n",
      "   1.21318756  0.89121825  1.51181913]\n",
      " [-0.28187572 -0.19935454 -0.36993625 -0.44277996 -0.36670803 -0.53757098\n",
      "  -0.68005798 -0.52156522 -0.80597949]\n",
      " [-0.69076062 -0.68745016 -0.71606603 -0.59998521 -0.56091087 -0.65765519\n",
      "  -0.85395179 -0.80571845 -0.87206148]\n",
      " [-0.85321498 -0.83331671 -0.92210586 -0.54965653 -0.49440111 -0.62592365\n",
      "  -0.6906597  -0.59278738 -0.76854549]\n",
      " [-0.54215083 -0.55714278 -0.56052752 -0.44570504 -0.42758279 -0.47894108\n",
      "  -0.52243003 -0.5210966  -0.51927992]\n",
      " [-0.383125   -0.4229195  -0.32466211 -0.46416884 -0.46474237 -0.47066702\n",
      "  -0.52594441 -0.59501973 -0.42369719]]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess features\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "print(scaler)\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (25, 9)\n",
      "Labels shape: (25,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (17, 9)\n",
      "X_test shape: (8, 9)\n",
      "y_train shape: (17,)\n",
      "y_test shape: (8,)\n",
      "Training class distribution: [ 0 17]\n",
      "Testing class distribution: [0 8]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Stratified splitting to maintain class balance\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state= 42)\n",
    "for train_idx, test_idx in splitter.split(features, labels):\n",
    "    X_train, X_test = features[train_idx], features[test_idx]\n",
    "    y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Verify class distribution\n",
    "print(\"Training class distribution:\", np.bincount(y_train))\n",
    "print(\"Testing class distribution:\", np.bincount(y_test))\n",
    "\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=7)\n",
    "# for train_idx, test_idx in splitter.split(features, labels):\n",
    "#     X_train, X_test = features[train_idx], features[test_idx]\n",
    "#     y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "\n",
    "# print(\"Training class distribution:\", np.bincount(y_train))\n",
    "# print(\"Testing class distribution:\", np.bincount(y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training class distribution:\", np.bincount(y_train))\n",
    "# print(\"Testing class distribution:\", np.bincount(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Sample X_train:\", X_train[:5])\n",
    "# print(\"Sample y_train:\", y_train[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplot\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m plot(f\u001b[38;5;241m=\u001b[39m\u001b[43mdata\u001b[49m[\u001b[38;5;241m35\u001b[39m],  plot_func\u001b[38;5;241m=\u001b[39msns\u001b[38;5;241m.\u001b[39mbarplot, t_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPressure\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "plot(f=data[35],  plot_func=sns.barplot, t_id=0, x='Timestamp', y='Pressure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split DaTA (Test and Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train-test split\n",
    "# np.random.seed(42)\n",
    "# pos_indices = np.where(labels == 1)[0]\n",
    "# neg_indices = np.where(labels == 0)[0]\n",
    "\n",
    "# np.random.shuffle(pos_indices)\n",
    "# np.random.shuffle(neg_indices)\n",
    "\n",
    "# train_pos = pos_indices[:-5]\n",
    "# test_pos = pos_indices[-5:]\n",
    "# train_neg = neg_indices[:-5]\n",
    "# test_neg = neg_indices[-5:]\n",
    "\n",
    "# train_indices = np.concatenate([train_pos, train_neg])\n",
    "# test_indices = np.concatenate([test_pos, test_neg])\n",
    "\n",
    "# X_train = features[train_indices]\n",
    "# y_train = labels[train_indices]\n",
    "# X_test = features[test_indices]\n",
    "# y_test = labels[test_indices]\n",
    "\n",
    "# print(\"Training set shape:\", X_train.shape)\n",
    "# print(\"Testing set shape:\", X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics function\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"{model_name} Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision * 100:.2f}%\")\n",
    "    print(f\"Recall: {recall * 100:.2f}%\")\n",
    "    print(f\"F1 Score: {f1 * 100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Splitting the dataset into positive and negative cases\n",
    "# pos_indices = np.where(labels == 1)[0]\n",
    "# neg_indices = np.where(labels == 0)[0]\n",
    "\n",
    "# # Shuffle indices to ensure randomness\n",
    "# np.random.shuffle(pos_indices)\n",
    "# np.random.shuffle(neg_indices)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# train_pos = pos_indices[:-5]\n",
    "# test_pos = pos_indices[-5:]\n",
    "# train_neg = neg_indices[:-5]\n",
    "# test_neg = neg_indices[-5:]\n",
    "\n",
    "# # Create training and testing datasets\n",
    "# train_indices = np.concatenate([train_pos, train_neg])\n",
    "# test_indices = np.concatenate([test_pos, test_neg])\n",
    "\n",
    "# X_train = features[train_indices]\n",
    "# y_train = labels[train_indices]\n",
    "# X_test = features[test_indices]\n",
    "# y_test = labels[test_indices]\n",
    "\n",
    "# print(\"Training set shape:\", X_train.shape)\n",
    "# print(\"Testing set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(prediction,actual):\n",
    "    correct = 0\n",
    "    not_correct = 0\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i] == actual[i]:\n",
    "            correct+=1\n",
    "        else:\n",
    "            not_correct+=1\n",
    "    return (correct*100)/(correct+not_correct)\n",
    "\n",
    "\n",
    "def metrics(prediction,actual):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i] == actual[i] and actual[i]==1:\n",
    "            tp+=1\n",
    "        if prediction[i] == actual[i] and actual[i]==0:\n",
    "            tn+=1\n",
    "        if prediction[i] != actual[i] and actual[i]==0:\n",
    "            fp+=1\n",
    "        if prediction[i] != actual[i] and actual[i]==1:\n",
    "            fn+=1\n",
    "    metrics = {'Precision':(tp/(tp+fp+tn+fn)),'Recall':(tp/(tp+fn)),'F1':(2*(tp/(tp+fp+tn+fn))*(tp/(tp+fn)))/((tp/(tp+fp+tn+fn))+(tp/(tp+fn)))}\n",
    "    return (metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "def train_logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    false_positive_rate = calculate_false_positive_rate(y_test, y_pred)\n",
    "    print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Logistic Regression False Positive Rate: {false_positive_rate * 100:.2f}%\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "def train_random_forest(X_train, y_train, X_test, y_test):\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    false_positive_rate = calculate_false_positive_rate(y_test, y_pred)\n",
    "    print(f\"Random Forest Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Random Forest False Positive Rate: {false_positive_rate * 100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train K-Nearest Neighbors\n",
    "def train_knn(X_train, y_train, X_test, y_test):\n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    false_positive_rate = calculate_false_positive_rate(y_test, y_pred)\n",
    "    print(f\"KNN Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"KNN False Positive Rate: {false_positive_rate * 100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Support Vector Machine\n",
    "def train_svm(X_train, y_train, X_test, y_test):\n",
    "    clf = SVC()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    false_positive_rate = calculate_false_positive_rate(y_test, y_pred)\n",
    "    print(f\"SVM Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"SVM False Positive Rate: {false_positive_rate * 100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost\n",
    "def train_xgboost(X_train, y_train, X_test, y_test):\n",
    "    clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    false_positive_rate = calculate_false_positive_rate(y_test, y_pred)\n",
    "    print(f\"XGBoost Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"XGBoost False Positive Rate: {false_positive_rate * 100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[215], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train and evaluate models separately\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_logistic_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m train_random_forest(X_train, y_train, X_test, y_test)\n\u001b[0;32m      4\u001b[0m train_knn(X_train, y_train, X_test, y_test)\n",
      "Cell \u001b[1;32mIn[210], line 4\u001b[0m, in \u001b[0;36mtrain_logistic_regression\u001b[1;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_logistic_regression\u001b[39m(X_train, y_train, X_test, y_test):\n\u001b[0;32m      3\u001b[0m     clf \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      6\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:1301\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1299\u001b[0m classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_classes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1305\u001b[0m     )\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1308\u001b[0m     n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(1)"
     ]
    }
   ],
   "source": [
    "# Train and evaluate models separately\n",
    "train_logistic_regression(X_train, y_train, X_test, y_test)\n",
    "train_random_forest(X_train, y_train, X_test, y_test)\n",
    "train_knn(X_train, y_train, X_test, y_test)\n",
    "train_svm(X_train, y_train, X_test, y_test)\n",
    "train_xgboost(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Classifier training and evaluation\n",
    "# classifiers = {\n",
    "#     \"Logistic Regression\": LogisticRegression(),\n",
    "#     \"Random Forest\": RandomForestClassifier(),\n",
    "#     \"Support Vector Machine\": SVC(),\n",
    "#     \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "#     \"XGBoost\": XGBClassifier(),\n",
    "#     \"Decision Tree\": DecisionTreeClassifier(),\n",
    "# }\n",
    "\n",
    "# for name, clf in classifiers.items():\n",
    "#     print(f\"Training {name}...\")\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     y_pred = clf.predict(X_test)\n",
    "#     evaluate_model(y_test, y_pred, name)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
